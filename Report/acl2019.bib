@incollection{distbelief,
title = {Large Scale Distributed Deep Networks},
author = {Jeffrey Dean and Greg Corrado and Rajat Monga and Chen, Kai and Matthieu Devin and Mark Mao and Marc\textquotesingle aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Quoc V. Le and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1223--1231},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}
}

@article{model_batch_domain_parallel,
  author    = {Amir Gholami and
               Ariful Azad and
               Kurt Keutzer and
               Aydin Bulu{\c{c}}},
  title     = {Integrated Model and Data Parallelism in Training Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1712.04432},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.04432},
  archivePrefix = {arXiv},
  eprint    = {1712.04432},
  timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-04432},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@inproceedings{rnn,
 author = {Collobert, Ronan and Weston, Jason},
 title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 series = {ICML '08},
 year = {2008},
 isbn = {978-1-60558-205-4},
 location = {Helsinki, Finland},
 pages = {160--167},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1390156.1390177},
 doi = {10.1145/1390156.1390177},
 acmid = {1390177},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{gcn,
  author    = {Mikael Henaff and
               Joan Bruna and
               Yann LeCun},
  title     = {Deep Convolutional Networks on Graph-Structured Data},
  journal   = {CoRR},
  volume    = {abs/1506.05163},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.05163},
  archivePrefix = {arXiv},
  eprint    = {1506.05163},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HenaffBL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{dl_book,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 title = {Deep Learning},
 year = {2016},
 isbn = {0262035618, 9780262035613},
 publisher = {The MIT Press},
} 

@incollection{training_time,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@article{beyond_data_model_parallelism,
  author    = {Zhihao Jia and
               Matei Zaharia and
               Alex Aiken},
  title     = {Beyond Data and Model Parallelism for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1807.05358},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.05358},
  archivePrefix = {arXiv},
  eprint    = {1807.05358},
  timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-05358},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{one_weird_trick_cnns,
  author    = {Alex Krizhevsky},
  title     = {One weird trick for parallelizing convolutional neural networks},
  journal   = {CoRR},
  volume    = {abs/1404.5997},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.5997},
  archivePrefix = {arXiv},
  eprint    = {1404.5997},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Krizhevsky14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{slow_learners_are_fast,
 author = {Langford, John and Smola, Alexander J. and Zinkevich, Martin},
 title = {Slow Learners Are Fast},
 booktitle = {Proceedings of the 22Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'09},
 year = {2009},
 isbn = {978-1-61567-911-9},
 location = {Vancouver, British Columbia, Canada},
 pages = {2331--2339},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2984093.2984354},
 acmid = {2984354},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@incollection{distributed_delayed_sgd,
title = {Distributed Delayed Stochastic Optimization},
author = {Agarwal, Alekh and Duchi, John C},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {873--881},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf}
}

@article{nedic,
title = "Distributed asynchronous incremental subgradient methods",
abstract = "We propose and analyze a distributed asynchronous subgradient method for minimizing a convex function that consists of the sum of a large number of component functions. This type of minimization arises in a dual context from Lagrangian relaxation of the coupling constraints of large scale separable problems. The idea is to distribute the computation of the component subgradients among a set of processors, which communicate only with a coordinator. The coordinator performs the subgradient iteration incrementally and asynchronously, by taking steps along the subgradients of the component funtions that are available at the update time. The incremental approach has performed very well in centralized computation, and the parallel implementation should improve its performance substantially, particularly for typical problems where computation of the component subgradients is relatively costly.",
author = "Angelia Nedich and Bertsekas, {D. P.} and Borkar, {V. S.}",
year = "2001",
doi = "10.1016/S1570-579X(01)80023-9",
language = "English (US)",
volume = "8",
pages = "381--407",
journal = "Studies in Computational Mathematics",
issn = "1570-579X",
publisher = "Elsevier",
number = "C",
}

@inproceedings{dl_using_gpus,
 author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
 title = {Large-scale Deep Unsupervised Learning Using Graphics Processors},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {873--880},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1553374.1553486},
 doi = {10.1145/1553374.1553486},
 acmid = {1553486},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{cuda,
 author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
 title = {Scalable Parallel Programming with CUDA},
 journal = {Queue},
 issue_date = {March/April 2008},
 volume = {6},
 number = {2},
 month = mar,
 year = {2008},
 issn = {1542-7730},
 pages = {40--53},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1365490.1365500},
 doi = {10.1145/1365490.1365500},
 acmid = {1365500},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@InProceedings{dl_using_gpus2,
author = {Deng, Li and Acero, Alex and Dahl, George and Yu, Dong},
title = {Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition},
booktitle = {IEEE Transactions on Audio, Speech, and Language Processing},
year = {2012},
month = {January},
abstract = {We propose a novel context-dependent (CD) model for large vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum likelihood (ML) criteria, respectively.},
url = {https://www.microsoft.com/en-us/research/publication/context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition/},
pages = {30-42},
volume = {20},
edition = {IEEE Transactions on Audio, Speech, and Language Processing (receiving 2013 IEEE SPS Best Paper Award)},
note = {IEEE SPS 2013 best paper award},
}

@misc{cuda_webinar,
  title = {{Optimizing Parallel Reduction in CUDA}, {\url{http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf}},
  note = {Accessed: 2019-11-15}
}}

@article{openimages,
  title={OpenImages: A public dataset for large-scale multi-label and multi-class image classification.},
  author={Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Veit, Andreas and Abu-El-Haija, Sami
    and Belongie, Serge and Cai, David and Feng, Zheyun and Ferrari, Vittorio and Gomes, Victor
    and Gupta, Abhinav and Narayanan, Dhyanesh and Sun, Chen and Chechik, Gal and Murphy, Kevin},
  journal={Dataset available from https://github.com/openimages},
  year={2016}
}

@inproceedings{wholesale_cust,
  title={An{\'a}lise do perfil do cliente Recheio e desenvolvimento de um sistema promocional},
  author={Nuno Gon√ßalo Costa Fernandes Marques de Abreu},
  year={2011}
}

@misc{choi_reductions,
  author = {Choi, Jee}, 
  title = {Web Lecture: Programming Models - CUDA IV},
  howpublished = {\url{https://jeewhanchoi.github.io/uocis631f19/lecture12.pptx}},
  note = {Accessed: 2019-10-30}
}

@misc{d_parallel,
    title={Measuring the Effects of Data Parallelism on Neural Network Training},
    author={Christopher J. Shallue and Jaehoon Lee and Joseph Antognini and Jascha Sohl-Dickstein and Roy Frostig and George E. Dahl},
    year={2018},
    eprint={1811.03600},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@incollection{LS_DN,
title = {Large Scale Distributed Deep Networks},
author = {Jeffrey Dean and Greg Corrado and Rajat Monga and Chen, Kai and Matthieu Devin and Mark Mao and Marc\textquotesingle aurelio Ranzato and Andrew Senior and Paul Tucker and Ke Yang and Quoc V. Le and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1223--1231},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}
}